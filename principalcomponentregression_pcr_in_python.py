# -*- coding: utf-8 -*-
"""PrincipalComponentRegression_PCR_in_Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zjVbs_XIbdWDmVVMyO6VAn2G4U1Cu7jv

# **Principal Component Regression**

https://scikit-learn.org/stable/auto_examples/cross_decomposition/ </br>
https://www.geeksforgeeks.org/principal-component-regression-pcr/ </br>
https://www.xlstat.com/en/solutions/features/principal-component-regression </br>
https://www.youtube.com/watch?v=SWfucxnOF8c

# **Principal component regression** *a statistical technique for regression analysis that is used to reduce the dimensionality of a dataset by projecting it onto a lower-dimensional subspace. This is done by finding a set of orthogonal (i.e., uncorrelated) linear combinations of the original variables, called principal components, that capture the most variance in the data. The principal components are used as predictors in the regression model, instead of the original variables.*

Note: An alternative to multiple linear regression, especially when the number of variables is large or when the variables are correlated
"""

#Import libraries / modules

from sklearn.datasets import load_diabetes
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.pipeline import Pipeline

X, y = load_diabetes(return_X_y=True)
X.shape

"""- *Reduce the dimensionality* of the original dataset by half that is from 10-dimensional data to 5-dimensional data.
- A *pipeline* with PCA and linear regression: A pipeline is created that consists of two steps: PCA and linear regression.
- The *PCA step* is initialised with the n_components parameter set to 6, which means that only the first six principal components will be kept.
- The *linear regression step* is initialised with the default parameters.
"""

# Create a pipeline with PCA and linear regression
pca = PCA(n_components=5)

# Keep only the first six principal components
reg = LinearRegression()

pipeline = Pipeline(steps=[('standardscaler', StandardScaler()),
                          ('pca', pca),
                           ('reg', reg)])

# Fit the pipeline to the data
pipeline.fit(X, y)

# Predict the labels for the data
y_pred = pipeline.predict(X)

# Compute the evaluation metrics
mae = mean_absolute_error(y, y_pred)
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)
r2 = pipeline.score(X, y)

# Print the number of features before and after PCR
print(f'Number of features before PCR: {X.shape[1]}')
print(f'Number of features after PCR: {pca.n_components_}')

# Print the evaluation metrics
print(f'MAE: {mae:.2f}') #Mean Absolute Error
print(f'MSE: {mse:.2f}') #Mean Squared Error
print(f'RMSE: {rmse:.2f}') #Root Mean Squared Error
print(f'R^2: {r2:.2f}') #R-squared value